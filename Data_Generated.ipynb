{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import  Dataset\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.io\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './segmented_data.pickle'\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "subject_names = [\n",
    "    'sensei-103', 'sensei-178', 'sensei-188', 'sensei-201', 'sensei-223', \n",
    "    'sensei-237', 'sensei-318', 'sensei-359', 'sensei-415', 'sensei-460', \n",
    "    'sensei-489', 'sensei-616', 'sensei-642', 'sensei-661', 'sensei-715', \n",
    "    'sensei-862', 'sensei-879', 'sensei-904', 'sensei-972', 'sensei-997'\n",
    "]\n",
    "subject_mapping = {name: i for i, name in enumerate(subject_names, start=1)}\n",
    "\n",
    "sensors = [\n",
    "    'corsano_wrist', 'cosinuss_ear', 'sensomative_bottom', 'sensomative_back', \n",
    "    'vivalink_patch', 'zurichmove_wheel'\n",
    "]\n",
    "\n",
    "all_sensors_data = []\n",
    "\n",
    "original_sampling_rate = 125\n",
    "new_sampling_rate = 20\n",
    "\n",
    "for subject, labels in data.items():\n",
    "    for label, sensors_data in labels.items():\n",
    "        sensor_dfs = []\n",
    "        for sensor in sensors:\n",
    "            if sensor in sensors_data:\n",
    "                df = sensors_data[sensor].copy()\n",
    "                if not df.empty:\n",
    "                    timestamps = np.linspace(0, len(df)/original_sampling_rate, len(df), endpoint=False)\n",
    "                    new_timestamps = np.arange(0, timestamps[-1], 1/new_sampling_rate)\n",
    "                    \n",
    "                    interpolated_sensor_data = pd.DataFrame()\n",
    "                    for column in df.columns:\n",
    "                        interpolator = interp1d(timestamps, df[column], kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "                        interpolated_sensor_data[column] = interpolator(new_timestamps)\n",
    "                    \n",
    "                    interpolated_sensor_data = interpolated_sensor_data.add_prefix(sensor + '_')\n",
    "                    sensor_dfs.append(interpolated_sensor_data)\n",
    "        if sensor_dfs:\n",
    "            concatenated_df = pd.concat(sensor_dfs, axis=1)\n",
    "            concatenated_df['label'] = label\n",
    "            concatenated_df['subject'] = subject_mapping[subject]\n",
    "            all_sensors_data.append(concatenated_df)\n",
    "\n",
    "final_combined_df = pd.concat(all_sensors_data, ignore_index=True)\n",
    "\n",
    "existing_subjects = sorted(final_combined_df['subject'].unique())\n",
    "new_subject_mapping = {old: new for new, old in enumerate(existing_subjects, start=1)}\n",
    "final_combined_df['subject'] = final_combined_df['subject'].map(new_subject_mapping)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cols_to_exclude = ['label', 'subject']\n",
    "features_df = final_combined_df.drop(columns=cols_to_exclude)\n",
    "standardized_features = scaler.fit_transform(features_df)\n",
    "standardized_features_df = pd.DataFrame(standardized_features, columns=features_df.columns)\n",
    "\n",
    "final_standardized_df = pd.concat([standardized_features_df, final_combined_df[cols_to_exclude].reset_index(drop=True)], axis=1)\n",
    "\n",
    "root_dir = 'segmented_data/combined_sensor_data.csv'\n",
    "final_standardized_df.to_csv(root_dir, index=False)\n",
    "\n",
    "df = pd.read_csv(root_dir)\n",
    "df.drop(columns=['vivalink_patch_ecg'], inplace=True)\n",
    "modified_csv_file_path = './segmented_data/modified_combined_sensor_data.csv'\n",
    "df.to_csv(modified_csv_file_path, index=False)\n",
    "pickle_file_path = './segmented_data/modified_combined_sensor_data.pkl'\n",
    "df.to_pickle(pickle_file_path)\n",
    "\n",
    "print(f\"DataFrame saved as Pickle file to {pickle_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MhealthDataset(Dataset): # 50hz   \n",
    "    '''L1: Standing still (1 min) \n",
    "    L2: Sitting and relaxing (1 min) \n",
    "    L3: Lying down (1 min) \n",
    "    L4: Walking (1 min) \n",
    "    L5: Climbing stairs (1 min) \n",
    "    L6: Waist bends forward (20x) \n",
    "    L7: Frontal elevation of arms (20x)\n",
    "    L8: Knees bending (crouching) (20x)\n",
    "    L9: Cycling (1 min)\n",
    "    L10: Jogging (1 min)\n",
    "    L11: Running (1 min)\n",
    "    L12: Jump front & back (20x)'''\n",
    "    def __init__(self, csv_path='./Datapool_new/Mhealth_data.csv', transform=None):\n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "\n",
    "            files = [f'./ADL/Dataset/1.Mhealth/MHEALTHDATASET/mHealth_subject{i}.log' for i in range(1, 11)]\n",
    "            columns = [0, 1, 2, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 18, 19, 23]\n",
    "            mhealth_column_names = ['acx_chest', 'acy_chest', 'acz_chest',\n",
    "                                    'acx_l_ankle', 'acy_l_ankle', 'acz_l_ankle',\n",
    "                                    'gyx_l_ankle', 'gyy_l_ankle', 'gyz_l_ankle',\n",
    "                                    'acx_r_arm', 'acy_r_arm', 'acz_r_arm',\n",
    "                                    'gyx_r_arm', 'gyy_r_arm', 'gyz_r_arm',\n",
    "                                    'label','subject']\n",
    "            mhealth_data = []\n",
    "\n",
    "            for i, file in enumerate(files, start=1):\n",
    "                data = np.loadtxt(file)\n",
    "                selected_data = data[:, columns[:-1]]\n",
    "                labels = data[:, columns[-1]] \n",
    "\n",
    "                timestamps = np.arange(selected_data.shape[0]) / 50.0\n",
    "                new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "                \n",
    "                downsampled_data = []\n",
    "                for j in range(selected_data.shape[1]):\n",
    "                    interpolator = interp1d(timestamps, selected_data[:, j], kind='linear')\n",
    "                    downsampled_column = interpolator(new_timestamps)\n",
    "                    downsampled_data.append(downsampled_column)\n",
    "                label_interpolator = interp1d(timestamps, labels, kind='nearest')\n",
    "                downsampled_labels = label_interpolator(new_timestamps)\n",
    "                downsampled_data_with_labels = np.column_stack((np.column_stack(downsampled_data), downsampled_labels))\n",
    "                \n",
    "                subject_column = np.full(downsampled_data_with_labels.shape[0], i, dtype=int)\n",
    "                downsampled_data_with_subject = np.column_stack((downsampled_data_with_labels, subject_column))\n",
    "                \n",
    "                mhealth_data.append(downsampled_data_with_subject)\n",
    "            mhealth_dataset = pd.DataFrame(np.vstack(mhealth_data), columns=mhealth_column_names)\n",
    "          \n",
    "            mhealth_dataset = mhealth_dataset[mhealth_dataset['label'] != 0]\n",
    "            cols_to_exclude = ['label','subject']\n",
    "            labels = mhealth_dataset['label'].values\n",
    "            subjects = mhealth_dataset['subject'].values\n",
    "            mhealth_to_standardize = mhealth_dataset.drop(columns=cols_to_exclude)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(mhealth_to_standardize)\n",
    "\n",
    "            features_df = pd.DataFrame(features, columns=mhealth_to_standardize.columns)\n",
    "            labels_df = pd.DataFrame(labels, columns=['label'])\n",
    "            subjects_df = pd.DataFrame(subjects, columns=['subject'])\n",
    "\n",
    "            processed_data = pd.concat([features_df, labels_df,subjects_df], axis=1)\n",
    "            for col in ['label', 'subject']:\n",
    "                processed_data[col] = processed_data[col].astype(int)\n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                print(1)\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        cols_to_exclude = ['label', 'subject']\n",
    "        self.labels = processed_data['label'].values\n",
    "        self.subjects = processed_data['subject'].values\n",
    "        self.features = processed_data.drop(columns=cols_to_exclude).values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        subject = self.subjects[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y, subject\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USCHADDataset(Dataset): #100hz\n",
    "    \n",
    "    '''1. Walking Forward\n",
    "    2. Walking Left\n",
    "    3. Walking Right\n",
    "    4. Walking Upstairs\n",
    "    5. Walking Downstairs\n",
    "    6. Running Forward\n",
    "    7. Jumping Up\n",
    "    8. Sitting\n",
    "    9. Standing\n",
    "    10. Sleeping\n",
    "    11. Elevator Up\n",
    "    12. Elevator Down'''\n",
    "    def __init__(self, root_dir='./ADL/Dataset/4.USC-HAD/', csv_path='./Datapool_new/USCHAD_data.csv', transform=None):\n",
    "        try:\n",
    "            self.processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "\n",
    "            self.data_frames = []\n",
    "            subject_id = 1\n",
    "            for subject_folder in sorted(os.listdir(root_dir)):\n",
    "                subject_folder_path = os.path.join(root_dir, subject_folder)\n",
    "                if os.path.isdir(subject_folder_path):\n",
    "                    # Process each MAT file within the subject folder\n",
    "                    for file in sorted(os.listdir(subject_folder_path)):\n",
    "                        file_path = os.path.join(subject_folder_path, file)\n",
    "                        if file.endswith('.mat'):\n",
    "                            mat = scipy.io.loadmat(file_path)\n",
    "                            sensor_readings = mat['sensor_readings']\n",
    "                            activity_number = mat['activity_number']\n",
    "\n",
    "                            timestamps = np.arange(len(sensor_readings)) / 100.0\n",
    "                            new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "                            downsampled_data = []\n",
    "                            for j in range(sensor_readings.shape[1]):\n",
    "                                interpolator = interp1d(timestamps, sensor_readings[:, j], kind='linear')\n",
    "                                downsampled_column = interpolator(new_timestamps)\n",
    "                                downsampled_data.append(downsampled_column)\n",
    "                            downsampled_data = np.array(downsampled_data).T\n",
    "                            \n",
    "                            activity_number = np.full(len(downsampled_data), activity_number[0])\n",
    "                            data_frame = pd.DataFrame(downsampled_data, columns=['accx', 'accy', 'accz', 'gyrx', 'gyry', 'gyrz'])\n",
    "\n",
    "                            data_frame['label'] = activity_number\n",
    "                            data_frame['subject'] = subject_id\n",
    "                            self.data_frames.append(data_frame)\n",
    "                    subject_id += 1\n",
    "            self.processed_data = pd.concat(self.data_frames, ignore_index=True)\n",
    "            \n",
    "            subjects = self.processed_data.pop('subject')\n",
    "            cols_to_exclude = ['label']\n",
    "            labels = self.processed_data[cols_to_exclude].values\n",
    "            features_to_standardize = self.processed_data.drop(columns=cols_to_exclude)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(features_to_standardize)\n",
    "\n",
    "            features_df = pd.DataFrame(features, columns=features_to_standardize.columns)\n",
    "            labels_df = pd.DataFrame(labels, columns=cols_to_exclude)\n",
    "\n",
    "            self.processed_data = pd.concat([features_df, labels_df, subjects], axis=1)\n",
    "            \n",
    "            for col in ['label', 'subject']:\n",
    "                self.processed_data[col] = self.processed_data[col].astype(int)\n",
    "            \n",
    "            print(self.processed_data['label'].min() )\n",
    "            if self.processed_data['label'].min() > 0:\n",
    "                print(1)\n",
    "                self.processed_data['label'] = self.processed_data['label'] - 1\n",
    "\n",
    "            self.processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = self.processed_data['label'].values\n",
    "        self.features = self.processed_data.drop(columns=['label', 'subject']).values\n",
    "        self.subjects = self.processed_data['subject'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        subject = self.subjects[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y, subject\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionsenseDataset(Dataset): # 50hz   \n",
    "    '''1.dws: downstairs\n",
    "    2.ups: upstairs\n",
    "    3.sit: sitting\n",
    "    4.std: standing\n",
    "    5.wlk: walking\n",
    "    6.jog: jogging''' \n",
    "    def __init__(self, root_dir='./ADL/Dataset/5.motionsense/A_DeviceMotion_data', csv_path='./Datapool_new/motionsense_data.csv', transform=None):\n",
    "        activities = {'dws': 1, 'ups': 2, 'sit': 3, 'std': 4, 'wlk': 5, 'jog': 6}\n",
    "        \n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            all_data = []\n",
    "\n",
    "            for activity, label in activities.items():\n",
    "                activity_folders = [f for f in os.listdir(root_dir) if f.startswith(activity)]\n",
    "                \n",
    "                for folder in activity_folders:\n",
    "                    folder_path = os.path.join(root_dir, folder)\n",
    "                    for file_name in os.listdir(folder_path)[:24]:\n",
    "                        subject_id = int(file_name.split('_')[1].split('.')[0])\n",
    "                        file_path = os.path.join(folder_path, file_name)\n",
    "                        data = pd.read_csv(file_path, usecols=[10, 11, 12, 7, 8, 9])\n",
    "                        \n",
    "                        timestamps = np.arange(len(data)) / 50.0\n",
    "                        new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "                        \n",
    "                        interpolated_data = []\n",
    "                        for column in data.columns:\n",
    "                            interpolator = interp1d(timestamps, data[column], kind='linear')\n",
    "                            downsampled_column = interpolator(new_timestamps)\n",
    "                            interpolated_data.append(downsampled_column)\n",
    "                        \n",
    "                        downsampled_data = np.column_stack(interpolated_data)\n",
    "                        labels = np.full((downsampled_data.shape[0], 1), label, dtype=int)\n",
    "                        subject_column = np.full((downsampled_data.shape[0], 1), subject_id, dtype=int)\n",
    "                        all_data.append(np.hstack((downsampled_data, labels, subject_column)))\n",
    "\n",
    "            all_data = np.vstack(all_data)\n",
    "            columns = ['accx', 'accy', 'accz', 'gyrx', 'gyry', 'gyrz', 'label', 'subject']\n",
    "            processed_data = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(processed_data.iloc[:, :-2])\n",
    "\n",
    "            processed_data.iloc[:, :-2] = features\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "            \n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = processed_data['label'].values\n",
    "        self.features = processed_data.drop(columns=['label']).values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAMAP2Dataset(Dataset): # 100hz   \n",
    "    '''1: 'lying',\n",
    "    2: 'sitting',\n",
    "    3: 'standing',\n",
    "    4: 'walking',\n",
    "    5: 'running',\n",
    "    6: 'cycling',\n",
    "    7: 'Nordic_walking',\n",
    "    8: 'ascending_stairs',\n",
    "    9: 'descending_stairs',\n",
    "    10: 'vacuum_cleaning',\n",
    "    11: 'ironing',\n",
    "    12: 'rope_jumping'''\n",
    "    def __init__(self, root_dir='./ADL/Dataset/6.PAMAP2/PAMAP2_Dataset/Protocol/', csv_path='./Datapool_new/PAMAP2_data.csv', transform=None):\n",
    "        try:\n",
    "            all_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            all_data = []\n",
    "\n",
    "            columns = [4, 5, 6, 10, 11, 12, 21, 22, 23, 27, 28, 29, 38, 39, 40, 44, 45, 46, 1]\n",
    "            column_names = ['handAccx', 'handAccy', 'handAccz', 'handGyrox', 'handGyroy', 'handGyroz',\n",
    "                            'chestAccx', 'chestAccy', 'chestAccz', 'chestGyrox', 'chestGyroy', 'chestGyroz',\n",
    "                            'ankleAccx', 'ankleAccy', 'ankleAccz', 'ankleGyrox', 'ankleGyroy', 'ankleGyroz','label']\n",
    "\n",
    "            subject_id = 1\n",
    "            for file_name in os.listdir(root_dir):\n",
    "                if file_name.endswith('.dat'):\n",
    "                    file_path = os.path.join(root_dir, file_name)\n",
    "                    data = pd.read_table(file_path, header=None, sep='\\s+')\n",
    "                    data = data[columns]\n",
    "                    data.columns = column_names\n",
    "                    data = data.apply(pd.to_numeric, errors='coerce')\n",
    "                    data = data[data['label'] != 0]\n",
    "                    data = data.interpolate()\n",
    "                    downsampled_data = data.iloc[::5].copy()\n",
    "                    downsampled_data['subject'] = subject_id\n",
    "\n",
    "                    all_data.append(downsampled_data)\n",
    "                    subject_id += 1\n",
    "                \n",
    "            all_data = pd.concat(all_data)\n",
    "\n",
    "            unique_labels = sorted(all_data['label'].unique())\n",
    "            label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels, start=1)}\n",
    "            all_data['label'] = all_data['label'].map(label_mapping)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features_to_scale = all_data.columns.drop(['label','subject'])\n",
    "            all_data[features_to_scale] = scaler.fit_transform(all_data[features_to_scale])\n",
    "\n",
    "            print(all_data['label'].min() )\n",
    "            if all_data['label'].min() > 0:\n",
    "                all_data['label'] = all_data['label'] - 1\n",
    "            all_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = all_data['label'].values\n",
    "        self.features = all_data.drop(columns=['label']).values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealdispDataset(Dataset): # 50hz   \n",
    "    \n",
    "    '''L1: Walking (1 min) L12: Waist rotation (20x) L23: Shoulders high amplitude rotation (20x)\n",
    "    L2: Jogging (1 min) L13: Waist bends (reach foot with opposite hand) (20x) L24: Shoulders low amplitude rotation (20x)\n",
    "    L3: Running (1 min) L14: Reach heels backwards (20x) L25: Arms inner rotation (20x)\n",
    "    L4: Jump up (20x) L15: Lateral bend (10x to the left + 10x to the right) L26: Knees (alternatively) to the breast (20x)\n",
    "    L5: Jump front & back (20x) L16: Lateral bend arm up (10x to the left + 10x to the right) L27: Heels (alternatively) to the backside (20x)\n",
    "    L6: Jump sideways (20x) L17: Repetitive forward stretching (20x) L28: Knees bending (crouching) (20x)\n",
    "    L7: Jump leg/arms open/closed (20x) L18: Upper trunk and lower body opposite twist (20x) L29: Knees (alternatively) bend forward (20x)\n",
    "    L8: Jump rope (20x) L19: Arms lateral elevation (20x) L30: Rotation on the knees (20x)\n",
    "    L9: Trunk twist (arms outstretched) (20x) L20: Arms frontal elevation (20x) L31: Rowing (1 min)\n",
    "    L10: Trunk twist (elbows bended) (20x) L21: Frontal hand claps (20x) L32: Elliptic bike (1 min)\n",
    "    L11: Waist bends forward (20x) L22: Arms frontal crossing (20x) L33: Cycling (1 min)'''\n",
    "    \n",
    "    def __init__(self, root_dir='./ADL/Dataset/7.REALDISP/', csv_path='./Datapool_new/Realdisp_data.csv', transform=None):\n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            \n",
    "            body_parts = ['RLA', 'RUA', 'BACK', 'LUA', 'LLA', 'RC', 'RT', 'LT', 'LC']\n",
    "            data_types = ['_accx', '_accy', '_accz', '_gyrx', '_gyry', '_gyrz']\n",
    "            column_names = [bp + dt for bp in body_parts for dt in data_types]\n",
    "            column_names.append('label')\n",
    "            column_names.append('subject')\n",
    "            all_data = []\n",
    "            for file_name in os.listdir(root_dir):\n",
    "                if 'ideal' in file_name and file_name.endswith('.log'):\n",
    "                    subject_id = int(file_name.split('subject')[1].split('_')[0])\n",
    "                    file_path = os.path.join(root_dir, file_name)\n",
    "                    columns_to_use = [2, 3, 4, 5, 6, 7, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 33, 41, 42, 43, 44, 45, 46, 54, 55, 56, 57, 58, 59, 67, 68, 69, 70, 71, 72, 80, 81, 82, 83, 84, 85, 93, 94, 95, 96, 97, 98, 106, 107, 108, 109, 110, 111, 119]\n",
    "                    data = pd.read_csv(file_path,sep='\\t', usecols=columns_to_use)\n",
    "                    timestamps = np.arange(len(data)) / 50.0\n",
    "                    new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "\n",
    "                    interpolated_data = []\n",
    "                    for column in data.columns[:-1]:\n",
    "                        interpolator = interp1d(timestamps, data[column], kind='linear')\n",
    "                        downsampled_column = interpolator(new_timestamps)\n",
    "                        interpolated_data.append(downsampled_column)\n",
    "\n",
    "                    label_interpolator = interp1d(timestamps, data.iloc[:, -1], kind='nearest')\n",
    "                    downsampled_labels = label_interpolator(new_timestamps)\n",
    "                    \n",
    "                    mask = downsampled_labels != 0\n",
    "                    downsampled_labels = downsampled_labels[mask]\n",
    "                    interpolated_data = [column[mask] for column in interpolated_data]\n",
    "                    \n",
    "                    subject_ids = np.full(len(downsampled_labels), subject_id)\n",
    "                    \n",
    "                    downsampled_data = np.column_stack((np.column_stack(interpolated_data), downsampled_labels, subject_ids))\n",
    "                    all_data.append(downsampled_data)\n",
    "\n",
    "            all_data = np.vstack(all_data)\n",
    "            processed_data = pd.DataFrame(all_data, columns=column_names)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features = processed_data.iloc[:, :-2]\n",
    "            processed_data.iloc[:, :-2] = scaler.fit_transform(features)\n",
    "\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "            \n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = processed_data.iloc[:, -2].values\n",
    "        self.features = processed_data.iloc[:, :-2].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealworldDataset(Dataset): # 50hz   \n",
    "    '''1.climbingdown\n",
    "    2.climbingup\n",
    "    3.jumping\n",
    "    4.lying\n",
    "    5.running\n",
    "    6.sitting\n",
    "    7.standing\n",
    "    8.walking''' \n",
    "    def __init__(self, root_dir='./ADL/Dataset/9.RealWorld/', csv_path='./Datapool_new/Realworld_data.csv', transform=None):\n",
    "        activities = {'climbingdown': 7, 'climbingup': 8, 'jumping': 1, 'lying': 2, 'running': 3, 'sitting': 4, 'standing': 5, 'walking': 6}\n",
    "        body_parts = ['chest', 'forearm', 'head', 'shin', 'thigh', 'upperarm', 'waist']\n",
    "        \n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            all_data = []\n",
    "\n",
    "            for subject_folder in os.listdir(root_dir):\n",
    "                subject_id = int(subject_folder.replace('proband', ''))\n",
    "                subject_path = os.path.join(root_dir, subject_folder)\n",
    "                if os.path.isdir(subject_path):\n",
    "                    for activity, label in activities.items():\n",
    "                        for part in body_parts:\n",
    "                            acc_file = f'acc_{activity}_{part}.csv'\n",
    "                            gyro_file = f'Gyroscope_{activity}_{part}.csv'\n",
    "                            acc_path = os.path.join(subject_path, acc_file)\n",
    "                            gyro_path = os.path.join(subject_path, gyro_file)\n",
    "\n",
    "                            if os.path.isfile(acc_path) and os.path.isfile(gyro_path):\n",
    "                                acc_data = pd.read_csv(acc_path, usecols=[2, 3, 4])\n",
    "                                gyro_data = pd.read_csv(gyro_path, usecols=[2, 3, 4])\n",
    "\n",
    "                                min_length = min(len(acc_data), len(gyro_data))\n",
    "                                shorter_data = pd.read_csv(acc_path, usecols=[1]) if len(acc_data) == min_length else pd.read_csv(gyro_path, usecols=[1])\n",
    "                                acc_data = acc_data.iloc[:min_length]\n",
    "                                gyro_data = gyro_data.iloc[:min_length]\n",
    "                                \n",
    "                                start_time = int(shorter_data.iloc[0])\n",
    "                                end_time = int(shorter_data.iloc[-1])\n",
    "                                total_time = (end_time - start_time) / 1000.0\n",
    "                                \n",
    "                                original_freq = min_length / total_time\n",
    "                                timestamps = np.arange(min_length) / original_freq\n",
    "                                new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "                                \n",
    "                                interpolated_acc = [interp1d(timestamps, acc_data.iloc[:, i], kind='linear')(new_timestamps) for i in range(acc_data.shape[1])]\n",
    "                                interpolated_gyro = [interp1d(timestamps, gyro_data.iloc[:, i], kind='linear')(new_timestamps) for i in range(gyro_data.shape[1])]\n",
    "\n",
    "                                combined_data = np.column_stack((interpolated_acc + interpolated_gyro))\n",
    "                                labels = np.full(len(combined_data), label)\n",
    "                                subject_column = np.full(len(combined_data), subject_id)\n",
    "                                all_data.append(np.column_stack((combined_data, labels, subject_column)))\n",
    "\n",
    "            all_data = np.vstack(all_data)\n",
    "            columns = ['accx', 'accy', 'accz', 'gyrx', 'gyry', 'gyrz', 'label', 'subject']\n",
    "            processed_data = pd.DataFrame(all_data, columns=columns)\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(processed_data.iloc[:, :-2])\n",
    "\n",
    "            processed_data.iloc[:, :-2] = features\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "            \n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = processed_data['label'].values\n",
    "        self.features = processed_data.drop(columns=['label']).values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_file(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    data = data[['Arrival_Time', 'x', 'y', 'z', 'User', 'Model', 'Device', 'gt']]\n",
    "    \n",
    "    return data\n",
    "def process_group(group, activities):\n",
    "    start_time = int(group['Arrival_Time'].iloc[0])\n",
    "    end_time = int(group['Arrival_Time'].iloc[-1])\n",
    "    total_time = (end_time - start_time) / 1000.0\n",
    "    original_freq = len(group) / total_time\n",
    "\n",
    "    timestamps = np.arange(len(group)) / original_freq\n",
    "\n",
    "    new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "\n",
    "    interpolated_sensor_data = [interp1d(timestamps, group[col], kind='linear', fill_value='extrapolate')(new_timestamps) for col in ['x', 'y', 'z']]\n",
    "\n",
    "    label_indices = np.round(np.linspace(0, len(group) - 1, len(new_timestamps))).astype(int)\n",
    "    labels = np.array([activities[act] for act in group['gt'].iloc[label_indices]])\n",
    "\n",
    "    return np.column_stack(interpolated_sensor_data), labels\n",
    "\n",
    "def process_and_merge_data(acc_file, gyro_file, activities, users):\n",
    "    data_acc = load_and_process_file(acc_file)\n",
    "    data_gyro = load_and_process_file(gyro_file)\n",
    "\n",
    "    data_acc = data_acc[data_acc['gt'].notna() & (data_acc['gt'] != 'null')]\n",
    "    data_gyro = data_gyro[data_gyro['gt'].notna() & (data_gyro['gt'] != 'null')]\n",
    "\n",
    "    merged_data = []\n",
    "    for (device, user), acc_group in data_acc.groupby(['Device', 'User']):\n",
    "        gyro_group = data_gyro[(data_gyro['Device'] == device) & (data_gyro['User'] == user)]\n",
    "\n",
    "        if not gyro_group.empty:\n",
    "            processed_acc, acc_labels = process_group(acc_group, activities)\n",
    "            processed_gyro, gyro_labels = process_group(gyro_group, activities)\n",
    "\n",
    "            min_length = min(len(processed_acc), len(processed_gyro))\n",
    "            combined_data = np.hstack((processed_acc[:min_length], processed_gyro[:min_length]))\n",
    "\n",
    "            labels = acc_labels if len(processed_acc) < len(processed_gyro) else gyro_labels\n",
    "            labels = labels[:min_length]\n",
    "\n",
    "            user_id = users[user]\n",
    "            user_column = np.full(min_length, user_id, dtype=int)\n",
    "\n",
    "            combined_data_with_labels = np.hstack((combined_data, labels[:, None], user_column[:, None]))\n",
    "            merged_data.append(combined_data_with_labels)\n",
    "\n",
    "    final_data = np.vstack(merged_data)\n",
    "\n",
    "    return final_data\n",
    "\n",
    "\n",
    "class HHARDataset(Dataset):\n",
    "    def __init__(self, root_dir='./ADL/Dataset/8.HHAR/', csv_path='./Datapool_new/HHAR_data.csv', transform=None):\n",
    "        activities = {'null': 0, 'stand': 1, 'sit': 2, 'walk': 3, 'stairsup': 4, 'stairsdown': 5, 'bike': 6}\n",
    "        users = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9}\n",
    "\n",
    "        try:\n",
    "            all_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            all_data = []\n",
    "            \n",
    "            for file_type in ['Phones', 'Watch']:\n",
    "                acc_file = os.path.join(root_dir, f'{file_type}_accelerometer.csv')\n",
    "                gyro_file = os.path.join(root_dir, f'{file_type}_gyroscope.csv')\n",
    "\n",
    "                combined_data = process_and_merge_data(acc_file, gyro_file, activities, users)\n",
    "                all_data.append(combined_data)\n",
    "\n",
    "            all_data = np.vstack(all_data)\n",
    "\n",
    "            columns = ['accx', 'accy', 'accz', 'gyrx', 'gyry', 'gyrz', 'label', 'subject']\n",
    "            processed_data = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            processed_data.iloc[:, :-2] = scaler.fit_transform(processed_data.iloc[:, :-2])\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "\n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                print(1)\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = processed_data[['label', 'subject']].values\n",
    "        self.features = processed_data.drop(columns=['label', 'subject']).values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_semicolon(x):\n",
    "    return x.replace(';', '') if isinstance(x, str) else x\n",
    "\n",
    "class WISDMDataset(Dataset): # 50hz   \n",
    "    '''1.Walking\n",
    "    2.Jogging\n",
    "    3.Upstairs\n",
    "    4.Downstairs\n",
    "    5.Sitting\n",
    "    6.Standing''' \n",
    "    def __init__(self, root_dir='./ADL/Dataset/11.WISDM/', csv_path='./Datapool_new/WISDM_data.csv', transform=None):\n",
    "        \n",
    "        activities = {'Walking': 1, 'Jogging': 2, 'Upstairs': 3, 'Downstairs': 4, 'Sitting': 5, 'Standing': 6}\n",
    "        \n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            \n",
    "            file_path = os.path.join(root_dir, 'WISDM_ar_v1.1_raw.txt')\n",
    "            raw_data = pd.read_csv(file_path, header=None, names=['subject', 'activity', 'timestamp', 'accx', 'accy', 'accz'],\n",
    "                        converters={'accx': remove_semicolon, 'accy': remove_semicolon, 'accz': remove_semicolon})\n",
    "            \n",
    "            raw_data['activity'] = raw_data['activity'].map(activities)\n",
    "            \n",
    "            all_data = []\n",
    "            \n",
    "            time_interval_threshold = 3600\n",
    "            \n",
    "            for user in range(1, 37):\n",
    "                for activity in activities.values():\n",
    "                    user_activity_data = raw_data[(raw_data['subject'] == user) & (raw_data['activity'] == activity)].copy()\n",
    "                    for col in ['timestamp', 'accx', 'accy', 'accz']:\n",
    "                        user_activity_data[col] = pd.to_numeric(user_activity_data[col], errors='coerce')\n",
    "                        user_activity_data[col].replace(0, np.nan, inplace=True)\n",
    "                    user_activity_data.dropna(inplace=True)\n",
    "\n",
    "                    if not user_activity_data.empty:\n",
    "                        timestamps = user_activity_data['timestamp'].values / 1e9\n",
    "                        timestamp_diffs = np.abs(np.diff(timestamps))\n",
    "                        indices = np.where(timestamp_diffs > time_interval_threshold)[0] + 1\n",
    "                        sequences = np.split(user_activity_data, indices)\n",
    "\n",
    "                        for seq in sequences:\n",
    "                            if len(seq) < 2:\n",
    "                                continue\n",
    "\n",
    "                            downsampled_data = []\n",
    "\n",
    "                            timestamps = seq['timestamp'].values / 1e9\n",
    "                            length = len(timestamps)\n",
    "                            original_freq = length / (timestamps[-1] - timestamps[0])\n",
    "                            timestamps = np.arange(length) / original_freq\n",
    "                            new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "\n",
    "                            for col in ['accx', 'accy', 'accz']:\n",
    "                                interpolator = interp1d(timestamps, seq[col], kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "                                downsampled_data.append(interpolator(new_timestamps))\n",
    "\n",
    "                            downsampled_data = np.column_stack(downsampled_data)\n",
    "                            labels = np.full(downsampled_data.shape[0], activity)\n",
    "                            subject_column = np.full(downsampled_data.shape[0], user)\n",
    "                            \n",
    "                            all_data.append(np.column_stack((downsampled_data, labels, subject_column)))\n",
    "                    \n",
    "            all_data = np.vstack(all_data)\n",
    "            columns = ['accx', 'accy', 'accz', 'label', 'subject']\n",
    "            processed_data = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(processed_data.iloc[:, :-2])\n",
    "\n",
    "            processed_data.iloc[:, :-2] = features\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "\n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                print(1)\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "                \n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "        self.labels = processed_data['label'].values\n",
    "        self.features = processed_data.drop(columns=['label', 'subject']).values\n",
    "        self.subjects = processed_data['subject'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        subject = self.subjects[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y, subject\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSADSDataset(Dataset): # 25hz   \n",
    "    '''1.sitting, \n",
    "    2.standing, \n",
    "    3.4.lying on back and on right side, \n",
    "    5.6.ascending and descending stairs, \n",
    "    7.standing in an elevator still \n",
    "    8.and moving around in an elevator, \n",
    "    9.walking in a parking lot, \n",
    "    10.11.walking on a treadmill with a speed of 4 km/h (in flat and 15 deg inclined positions)\n",
    "    12.running on a treadmill with a speed of 8 km/h, \n",
    "    13.exercising on a stepper, \n",
    "    14.exercising on a cross trainer, \n",
    "    15.16.cycling on an exercise bike in horizontal and vertical positions,\n",
    "    17.rowing, \n",
    "    18.jumping, \n",
    "    19.playing basketball''' \n",
    "    def __init__(self, root_dir='./ADL/Dataset/12.UCI DSADS/data', csv_path='./Datapool_new/DSADS_data.csv', transform=None):\n",
    "        \n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            \n",
    "            all_data = []\n",
    "            for activity in range(1, 20):\n",
    "                activity_folder = f\"a{activity:02d}\"\n",
    "                for person in range(1, 9):\n",
    "                    person_folder = f\"p{person}\"\n",
    "                    path = os.path.join(root_dir, activity_folder, person_folder)\n",
    "\n",
    "                    for filename in os.listdir(path):\n",
    "                        file_path = os.path.join(path, filename)\n",
    "                        df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "                        selected_columns = df.iloc[:, np.r_[:6, 9:15, 18:24, 27:33, 36:42]]\n",
    "                        timestamps = np.arange(len(selected_columns)) / 25.0 \n",
    "                        new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "\n",
    "                        interpolated_data = []\n",
    "                        for col in selected_columns.columns:\n",
    "                            interpolator = interp1d(timestamps, selected_columns[col], kind='linear', fill_value='extrapolate')\n",
    "                            interpolated_data.append(interpolator(new_timestamps))\n",
    "\n",
    "                        interpolated_data = np.column_stack(interpolated_data)\n",
    "                        subject_column = np.full(interpolated_data.shape[0], person)\n",
    "                        activity_column = np.full(interpolated_data.shape[0], activity)\n",
    "                        all_data.append(np.column_stack((interpolated_data, activity_column, subject_column)))\n",
    "\n",
    "            all_data = np.vstack(all_data)\n",
    "            units = ['T', 'RA', 'LA', 'RL', 'LL']\n",
    "            columns = [f'{unit}_{col}' for unit in units for col in ['accx', 'accy', 'accz', 'gyrx', 'gyry', 'gyrz']] + ['label', 'subject']\n",
    "            processed_data = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features = processed_data.iloc[:, :-2]\n",
    "            processed_data.iloc[:, :-2] = scaler.fit_transform(features)\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "            \n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                print(1)\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = processed_data['label'].values\n",
    "        self.features = processed_data.drop(columns=['label', 'subject']).values\n",
    "        self.subjects = processed_data['subject'].values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        subject = self.subjects[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y, subject\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniMiBSHARDataset(Dataset): # 50hz   \n",
    "    '''1.standing_up\n",
    "    2.getting_up\n",
    "    3.walking\n",
    "    4.running\n",
    "    5.going_up\n",
    "    6.jumping\n",
    "    7.going_down\n",
    "    8.lying_down\n",
    "    9.sitting_down''' \n",
    "    def __init__(self, root_dir='./ADL/Dataset/13.UniMiB SHAR/data', csv_path='./Datapool_new/UniMiBSHAR_data.csv', transform=None):\n",
    "        \n",
    "        activities = {'Walking': 1, 'Running': 2, 'GoingUpS': 3, 'GoingDownS': 4, 'Jumping': 5, 'SittingDown': 6, 'StandingUpFS': 7, 'LyingDownFS': 8, 'StandingUpFL': 9}\n",
    "        \n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "            \n",
    "            mat_data = loadmat(os.path.join(root_dir, 'full_data.mat'))['full_data']\n",
    "            all_data = []\n",
    "            for subject in range(30):\n",
    "                subject_struct = mat_data[subject, 0]\n",
    "                for activity_name, activity_label in activities.items():\n",
    "                    activity_data_cells = subject_struct[activity_name][0][0]\n",
    "                    for trial in range(2): \n",
    "                        trial_data = activity_data_cells[trial, 0]\n",
    "                        acc_data = trial_data[:3, :].T\n",
    "                        timestamps = np.arange(acc_data.shape[0]) / 50.0\n",
    "                        new_timestamps = np.arange(0, timestamps[-1], 1/20.0)\n",
    "                        interpolated_data = []\n",
    "                        for col in range(acc_data.shape[1]):\n",
    "                            interpolator = interp1d(timestamps, acc_data[:, col], kind='linear', fill_value='extrapolate')\n",
    "                            interpolated_data.append(interpolator(new_timestamps))\n",
    "                        interpolated_data = np.column_stack(interpolated_data)\n",
    "\n",
    "                        label_column = np.full(interpolated_data.shape[0], activity_label)\n",
    "                        subject_column = np.full(interpolated_data.shape[0], subject + 1)\n",
    "                        all_data.append(np.column_stack((interpolated_data, label_column, subject_column)))\n",
    "\n",
    "            all_data = np.vstack(all_data)\n",
    "            columns = ['accx', 'accy', 'accz', 'label', 'subject']\n",
    "            processed_data = pd.DataFrame(all_data, columns=columns)\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(processed_data.iloc[:, :-2])\n",
    "\n",
    "            processed_data.iloc[:, :-2] = features\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "\n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                print(1)\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = processed_data['label'].values\n",
    "        self.features = processed_data.drop(columns=['label', 'subject']).values\n",
    "        self.subjects = processed_data['subject'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        subject = self.subjects[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y, subject\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WARDDataset(Dataset): # 50hz   \n",
    "    '''1. Stand\n",
    "    2. Sit\n",
    "    3. Lie down\n",
    "    4. Walk forward\n",
    "    5. Walk left-circle\n",
    "    6. Walk right-circle\n",
    "    7. Turn left\n",
    "    8. Turn right\n",
    "    9. Go upstairs\n",
    "    10. Go downstairs\n",
    "    11. Jog\n",
    "    12. Jump\n",
    "    13. Push wheelchair''' \n",
    "    def __init__(self, root_dir='./ADL/Dataset/14.WARD/', csv_path='./Datapool_new/WARD_data.csv', transform=None):\n",
    "        \n",
    "        sensor_prefixes = ['LF_', 'RF_', 'WAIST_', 'LA_', 'RA_']\n",
    "        feature_suffixes = ['accx', 'accy', 'accz', 'gyrx', 'gyry']\n",
    "        column_names = [prefix + suffix for prefix in sensor_prefixes for suffix in feature_suffixes]\n",
    "\n",
    "        try:\n",
    "            processed_data = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded data from {csv_path}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Processed file not found at {csv_path}. Processing data...\")\n",
    "\n",
    "            all_data = []\n",
    "            for subject in range(1, 21):\n",
    "                subject_folder = os.path.join(root_dir, f\"Subject{subject}\")\n",
    "                for mat_file in os.listdir(subject_folder):\n",
    "                    if mat_file.endswith('.mat'):\n",
    "                        mat_data = loadmat(os.path.join(subject_folder, mat_file))\n",
    "                        readings = mat_data['WearableData'][0][0][5][0]\n",
    "                        activity_data = np.hstack([readings[i] for i in range(5)])\n",
    "                        label = int(mat_file.split('a')[1].split('t')[0])\n",
    "                        subject_data = np.full((activity_data.shape[0], 1), subject)\n",
    "                        label_data = np.full((activity_data.shape[0], 1), label)\n",
    "\n",
    "                        combined_data = np.hstack((activity_data, label_data, subject_data))\n",
    "                        if np.isinf(combined_data).any():\n",
    "                            combined_data = combined_data[~np.isinf(combined_data).any(axis=1)]\n",
    "                        all_data.append(combined_data)\n",
    "\n",
    "            all_data = np.vstack(all_data)\n",
    "            columns = column_names + ['label', 'subject']\n",
    "            processed_data = pd.DataFrame(all_data, columns=columns)\n",
    "            scaler = StandardScaler()\n",
    "            features = scaler.fit_transform(processed_data.iloc[:, :-2])\n",
    "\n",
    "            processed_data.iloc[:, :-2] = features\n",
    "            processed_data['label'] = processed_data['label'].astype(int)\n",
    "            processed_data['subject'] = processed_data['subject'].astype(int)\n",
    "\n",
    "            print(processed_data['label'].min() )\n",
    "            if processed_data['label'].min() > 0:\n",
    "                print(1)\n",
    "                processed_data['label'] = processed_data['label'] - 1\n",
    "            processed_data.to_csv(csv_path, index=False)\n",
    "            print(f\"Processed data saved to {csv_path}\")\n",
    "\n",
    "        self.labels = processed_data['label'].values\n",
    "        self.features = processed_data.drop(columns=['label', 'subject']).values\n",
    "        self.subjects = processed_data['subject'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        subject = self.subjects[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y, subject\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhealth_dataset = MhealthDataset()\n",
    "USCHAD_dataset = USCHADDataset()\n",
    "Motionsense_dataset = MotionsenseDataset()\n",
    "PAMAP2_dataset = PAMAP2Dataset()\n",
    "Realdisp_dataset=RealdispDataset()\n",
    "Realworld_dataset=RealworldDataset()\n",
    "HHAR_Dataset = HHARDataset()\n",
    "WISDM_Dataset = WISDMDataset()\n",
    "DSADS_Dataset=DSADSDataset()\n",
    "UniMiBSHAR_Dataset=UniMiBSHARDataset()\n",
    "WARD_Dataset = WARDDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './Datapool_new/'\n",
    "for filename in os.listdir(root_dir):\n",
    "\n",
    "    if filename.endswith('.csv'):\n",
    "        csv_file = os.path.join(root_dir, filename)\n",
    "\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        pickle_file = os.path.join(root_dir, os.path.splitext(filename)[0] + '.pkl')\n",
    "\n",
    "        df.to_pickle(pickle_file)\n",
    "\n",
    "        print(f\"Converted {csv_file} to {pickle_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
